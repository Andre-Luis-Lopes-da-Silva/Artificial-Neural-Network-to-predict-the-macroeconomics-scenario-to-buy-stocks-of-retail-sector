{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49579ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 20:08:39.448677: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-11 20:08:42.979184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-11 20:08:42.979212: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-11 20:08:53.250890: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-11 20:08:53.251083: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-11 20:08:53.251094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import modules. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398b6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file containing the dataset.\n",
    "dataset = pd.read_csv(\"/home/andre/Desktop/projeto Renner/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eea49e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the periods by comma. \n",
    "dataset['IBC-Br'] = [float(str(i).replace(\",\", \".\")) for i in dataset['IBC-Br']]\n",
    "dataset['icom'] = [float(str(i).replace(\",\", \".\")) for i in dataset['icom']]\n",
    "dataset['dolar'] = dataset['dolar'].str.replace(',','.')\n",
    "dataset['expectativa'] = dataset['expectativa'].str.replace(',','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b7ffc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 17)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a tuple representing the dimensionality of the DataFrame.\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c1216d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 132 entries, 0 to 131\n",
      "Data columns (total 17 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   date         132 non-null    object \n",
      " 1   trend        132 non-null    object \n",
      " 2   ipca         132 non-null    float64\n",
      " 3   selic        132 non-null    float64\n",
      " 4   IBC-Br       132 non-null    float64\n",
      " 5   icom         132 non-null    float64\n",
      " 6   expectativa  132 non-null    object \n",
      " 7   dolar        132 non-null    object \n",
      " 8   icon         132 non-null    float64\n",
      " 9   ibov         132 non-null    int64  \n",
      " 10  open         132 non-null    float64\n",
      " 11  high         132 non-null    float64\n",
      " 12  low          132 non-null    float64\n",
      " 13  close        132 non-null    float64\n",
      " 14  adjclose     132 non-null    float64\n",
      " 15  volume       132 non-null    int64  \n",
      " 16  ticker       132 non-null    object \n",
      "dtypes: float64(10), int64(2), object(5)\n",
      "memory usage: 17.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Prints information about the DataFrame.\n",
    "dataset.info()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa17367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipca</th>\n",
       "      <th>selic</th>\n",
       "      <th>IBC-Br</th>\n",
       "      <th>icom</th>\n",
       "      <th>icon</th>\n",
       "      <th>ibov</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>1.320000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.494015</td>\n",
       "      <td>8.861671</td>\n",
       "      <td>139.508712</td>\n",
       "      <td>91.899242</td>\n",
       "      <td>3152.596364</td>\n",
       "      <td>72257.000000</td>\n",
       "      <td>23.220766</td>\n",
       "      <td>25.028803</td>\n",
       "      <td>21.541370</td>\n",
       "      <td>23.326701</td>\n",
       "      <td>21.289157</td>\n",
       "      <td>1.123960e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.349057</td>\n",
       "      <td>3.652107</td>\n",
       "      <td>6.076853</td>\n",
       "      <td>12.156991</td>\n",
       "      <td>1090.243310</td>\n",
       "      <td>23045.457373</td>\n",
       "      <td>13.500074</td>\n",
       "      <td>14.484806</td>\n",
       "      <td>12.398052</td>\n",
       "      <td>13.420894</td>\n",
       "      <td>13.123618</td>\n",
       "      <td>5.040229e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.380000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>118.350000</td>\n",
       "      <td>61.200000</td>\n",
       "      <td>1498.440000</td>\n",
       "      <td>40406.000000</td>\n",
       "      <td>7.938352</td>\n",
       "      <td>8.736943</td>\n",
       "      <td>7.192235</td>\n",
       "      <td>7.936712</td>\n",
       "      <td>6.187788</td>\n",
       "      <td>1.766306e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>135.822500</td>\n",
       "      <td>85.525000</td>\n",
       "      <td>2391.112500</td>\n",
       "      <td>54221.000000</td>\n",
       "      <td>11.106888</td>\n",
       "      <td>12.222783</td>\n",
       "      <td>10.737519</td>\n",
       "      <td>11.214296</td>\n",
       "      <td>9.704381</td>\n",
       "      <td>8.081030e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>8.705645</td>\n",
       "      <td>139.300000</td>\n",
       "      <td>94.050000</td>\n",
       "      <td>2854.810000</td>\n",
       "      <td>63791.500000</td>\n",
       "      <td>18.406941</td>\n",
       "      <td>19.533496</td>\n",
       "      <td>17.226274</td>\n",
       "      <td>18.763602</td>\n",
       "      <td>16.814088</td>\n",
       "      <td>9.810343e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.742500</td>\n",
       "      <td>11.801613</td>\n",
       "      <td>143.170000</td>\n",
       "      <td>102.150000</td>\n",
       "      <td>3812.875000</td>\n",
       "      <td>90616.000000</td>\n",
       "      <td>34.152985</td>\n",
       "      <td>36.457018</td>\n",
       "      <td>30.689229</td>\n",
       "      <td>33.970031</td>\n",
       "      <td>31.415106</td>\n",
       "      <td>1.264414e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.350000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>152.130000</td>\n",
       "      <td>111.400000</td>\n",
       "      <td>5497.850000</td>\n",
       "      <td>126802.000000</td>\n",
       "      <td>56.995518</td>\n",
       "      <td>60.418224</td>\n",
       "      <td>55.725643</td>\n",
       "      <td>56.995518</td>\n",
       "      <td>53.631706</td>\n",
       "      <td>2.767085e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ipca       selic      IBC-Br        icom         icon  \\\n",
       "count  132.000000  132.000000  132.000000  132.000000   132.000000   \n",
       "mean     0.494015    8.861671  139.508712   91.899242  3152.596364   \n",
       "std      0.349057    3.652107    6.076853   12.156991  1090.243310   \n",
       "min     -0.380000    2.000000  118.350000   61.200000  1498.440000   \n",
       "25%      0.250000    6.500000  135.822500   85.525000  2391.112500   \n",
       "50%      0.450000    8.705645  139.300000   94.050000  2854.810000   \n",
       "75%      0.742500   11.801613  143.170000  102.150000  3812.875000   \n",
       "max      1.350000   14.250000  152.130000  111.400000  5497.850000   \n",
       "\n",
       "                ibov        open        high         low       close  \\\n",
       "count     132.000000  132.000000  132.000000  132.000000  132.000000   \n",
       "mean    72257.000000   23.220766   25.028803   21.541370   23.326701   \n",
       "std     23045.457373   13.500074   14.484806   12.398052   13.420894   \n",
       "min     40406.000000    7.938352    8.736943    7.192235    7.936712   \n",
       "25%     54221.000000   11.106888   12.222783   10.737519   11.214296   \n",
       "50%     63791.500000   18.406941   19.533496   17.226274   18.763602   \n",
       "75%     90616.000000   34.152985   36.457018   30.689229   33.970031   \n",
       "max    126802.000000   56.995518   60.418224   55.725643   56.995518   \n",
       "\n",
       "         adjclose        volume  \n",
       "count  132.000000  1.320000e+02  \n",
       "mean    21.289157  1.123960e+08  \n",
       "std     13.123618  5.040229e+07  \n",
       "min      6.187788  1.766306e+07  \n",
       "25%      9.704381  8.081030e+07  \n",
       "50%     16.814088  9.810343e+07  \n",
       "75%     31.415106  1.264414e+08  \n",
       "max     53.631706  2.767085e+08  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate descriptive statistics from dataframe.\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c81f2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the columns do not needed.\n",
    "new_data_set = dataset.drop(columns=['date', 'open', 'high', 'low', 'close', 'adjclose', 'volume', 'ticker'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "725d50da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting object type column to float.\n",
    "dataset['dolar'] = dataset['dolar'].astype(float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981ec9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trend</th>\n",
       "      <th>ipca</th>\n",
       "      <th>selic</th>\n",
       "      <th>IBC-Br</th>\n",
       "      <th>icom</th>\n",
       "      <th>expectativa</th>\n",
       "      <th>dolar</th>\n",
       "      <th>icon</th>\n",
       "      <th>ibov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uptrend</td>\n",
       "      <td>0.83</td>\n",
       "      <td>10.943548</td>\n",
       "      <td>132.66</td>\n",
       "      <td>106.1</td>\n",
       "      <td>163.71</td>\n",
       "      <td>1.67411428571429</td>\n",
       "      <td>1525.61</td>\n",
       "      <td>66575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Uptrend</td>\n",
       "      <td>0.80</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>136.18</td>\n",
       "      <td>111.4</td>\n",
       "      <td>166.17</td>\n",
       "      <td>1.66719</td>\n",
       "      <td>1533.05</td>\n",
       "      <td>67383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Uptrend</td>\n",
       "      <td>0.79</td>\n",
       "      <td>11.717742</td>\n",
       "      <td>144.93</td>\n",
       "      <td>105.3</td>\n",
       "      <td>162.43</td>\n",
       "      <td>1.6583</td>\n",
       "      <td>1629.59</td>\n",
       "      <td>68587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Uptrend</td>\n",
       "      <td>0.77</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>139.89</td>\n",
       "      <td>108.5</td>\n",
       "      <td>163.01</td>\n",
       "      <td>1.58564736842105</td>\n",
       "      <td>1672.59</td>\n",
       "      <td>66133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Uptrend</td>\n",
       "      <td>0.47</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>143.23</td>\n",
       "      <td>106.4</td>\n",
       "      <td>155.6</td>\n",
       "      <td>1.61269090909091</td>\n",
       "      <td>1636.41</td>\n",
       "      <td>64620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     trend  ipca      selic  IBC-Br   icom expectativa             dolar  \\\n",
       "0  Uptrend  0.83  10.943548  132.66  106.1      163.71  1.67411428571429   \n",
       "1  Uptrend  0.80  11.250000  136.18  111.4      166.17           1.66719   \n",
       "2  Uptrend  0.79  11.717742  144.93  105.3      162.43            1.6583   \n",
       "3  Uptrend  0.77  11.800000  139.89  108.5      163.01  1.58564736842105   \n",
       "4  Uptrend  0.47  12.000000  143.23  106.4       155.6  1.61269090909091   \n",
       "\n",
       "      icon   ibov  \n",
       "0  1525.61  66575  \n",
       "1  1533.05  67383  \n",
       "2  1629.59  68587  \n",
       "3  1672.59  66133  \n",
       "4  1636.41  64620  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first five registers from dataframe.\n",
    "new_data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8f78549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize labels. to transform non-numerical labels (as long as they are hashable and comparable) to numerical labels.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()  \n",
    "y = labelencoder.fit_transform(new_data_set['trend'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49650ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ipca</th>\n",
       "      <th>selic</th>\n",
       "      <th>IBC-Br</th>\n",
       "      <th>icom</th>\n",
       "      <th>expectativa</th>\n",
       "      <th>dolar</th>\n",
       "      <th>icon</th>\n",
       "      <th>ibov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.83</td>\n",
       "      <td>10.943548</td>\n",
       "      <td>132.66</td>\n",
       "      <td>106.1</td>\n",
       "      <td>163.71</td>\n",
       "      <td>1.67411428571429</td>\n",
       "      <td>1525.61</td>\n",
       "      <td>66575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.80</td>\n",
       "      <td>11.250000</td>\n",
       "      <td>136.18</td>\n",
       "      <td>111.4</td>\n",
       "      <td>166.17</td>\n",
       "      <td>1.66719</td>\n",
       "      <td>1533.05</td>\n",
       "      <td>67383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.79</td>\n",
       "      <td>11.717742</td>\n",
       "      <td>144.93</td>\n",
       "      <td>105.3</td>\n",
       "      <td>162.43</td>\n",
       "      <td>1.6583</td>\n",
       "      <td>1629.59</td>\n",
       "      <td>68587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.77</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>139.89</td>\n",
       "      <td>108.5</td>\n",
       "      <td>163.01</td>\n",
       "      <td>1.58564736842105</td>\n",
       "      <td>1672.59</td>\n",
       "      <td>66133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.47</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>143.23</td>\n",
       "      <td>106.4</td>\n",
       "      <td>155.6</td>\n",
       "      <td>1.61269090909091</td>\n",
       "      <td>1636.41</td>\n",
       "      <td>64620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>0.87</td>\n",
       "      <td>5.120968</td>\n",
       "      <td>142.05</td>\n",
       "      <td>100.9</td>\n",
       "      <td>143.86</td>\n",
       "      <td>5.25112272727273</td>\n",
       "      <td>5007.44</td>\n",
       "      <td>118781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1.16</td>\n",
       "      <td>5.516667</td>\n",
       "      <td>138.96</td>\n",
       "      <td>94.1</td>\n",
       "      <td>148.22</td>\n",
       "      <td>5.27909047619048</td>\n",
       "      <td>4709.56</td>\n",
       "      <td>110979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>1.25</td>\n",
       "      <td>6.443548</td>\n",
       "      <td>138.46</td>\n",
       "      <td>94.2</td>\n",
       "      <td>142.89</td>\n",
       "      <td>5.539375</td>\n",
       "      <td>4251.43</td>\n",
       "      <td>103501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>0.95</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>140.22</td>\n",
       "      <td>88.0</td>\n",
       "      <td>142.22</td>\n",
       "      <td>5.55626</td>\n",
       "      <td>3932.70</td>\n",
       "      <td>101915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.73</td>\n",
       "      <td>8.862903</td>\n",
       "      <td>141.96</td>\n",
       "      <td>85.3</td>\n",
       "      <td>143.51</td>\n",
       "      <td>5.65060434782609</td>\n",
       "      <td>3932.12</td>\n",
       "      <td>104822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ipca      selic  IBC-Br   icom expectativa             dolar     icon  \\\n",
       "0    0.83  10.943548  132.66  106.1      163.71  1.67411428571429  1525.61   \n",
       "1    0.80  11.250000  136.18  111.4      166.17           1.66719  1533.05   \n",
       "2    0.79  11.717742  144.93  105.3      162.43            1.6583  1629.59   \n",
       "3    0.77  11.800000  139.89  108.5      163.01  1.58564736842105  1672.59   \n",
       "4    0.47  12.000000  143.23  106.4       155.6  1.61269090909091  1636.41   \n",
       "..    ...        ...     ...    ...         ...               ...      ...   \n",
       "127  0.87   5.120968  142.05  100.9      143.86  5.25112272727273  5007.44   \n",
       "128  1.16   5.516667  138.96   94.1      148.22  5.27909047619048  4709.56   \n",
       "129  1.25   6.443548  138.46   94.2      142.89          5.539375  4251.43   \n",
       "130  0.95   7.750000  140.22   88.0      142.22           5.55626  3932.70   \n",
       "131  0.73   8.862903  141.96   85.3      143.51  5.65060434782609  3932.12   \n",
       "\n",
       "       ibov  \n",
       "0     66575  \n",
       "1     67383  \n",
       "2     68587  \n",
       "3     66133  \n",
       "4     64620  \n",
       "..      ...  \n",
       "127  118781  \n",
       "128  110979  \n",
       "129  103501  \n",
       "130  101915  \n",
       "131  104822  \n",
       "\n",
       "[132 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the macroeconomic indicators.\n",
    "previsores = new_data_set.drop(['trend'], axis = 1)\n",
    "X = previsores.iloc[:,0:132].values\n",
    "previsores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c960c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform features by scaling each feature to a given range.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "999aaa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69942197, 0.73008558, 0.42362345, 0.89442231, 0.89053295,\n",
       "       0.02715036, 0.0067935 , 0.30289597])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extracting a value from predictors\n",
    "d = X[0] \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76a5b87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((99, 8),\n",
       " array([0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "        0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "        1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split arrays or matrices into random train and test subsets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "X_train.shape, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8498cd24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((33, 8),\n",
       " array([1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the matrix shape and the y data for test. \n",
    "X_test.shape, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9490d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape. It's the starting tensor you send to the first hidden layer. This tensor must have the same shape as your training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c492204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-11 20:09:16.985398: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-11 20:09:16.985431: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-11 20:09:16.985457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (andre-Aspire-E1-572): /proc/driver/nvidia/version does not exist\n",
      "2023-01-11 20:09:17.003259: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Network architecture\n",
    "rede_neural = tf.keras.models.Sequential() # Sequential groups a linear stack of layers into a tf.keras.Model.\n",
    "rede_neural.add(tf.keras.layers.Dense(units = 4, activation='relu', input_shape=(8,)))\n",
    "rede_neural.add(tf.keras.layers.Dense(units = 4, activation='relu'))\n",
    "rede_neural.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e437124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 4)                 36        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 20        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61\n",
      "Trainable params: 61\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rede_neural.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3b7a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "rede_neural.compile(optimizer=opt, loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c92dc17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "4/4 [==============================] - 1s 5ms/step - loss: 0.6915 - accuracy: 0.6061\n",
      "Epoch 2/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.6364\n",
      "Epoch 3/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6846 - accuracy: 0.6364\n",
      "Epoch 4/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6821 - accuracy: 0.6364\n",
      "Epoch 5/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6790 - accuracy: 0.6364\n",
      "Epoch 6/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6760 - accuracy: 0.6465\n",
      "Epoch 7/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6724 - accuracy: 0.6566\n",
      "Epoch 8/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.6667\n",
      "Epoch 9/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6654 - accuracy: 0.6768\n",
      "Epoch 10/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6611 - accuracy: 0.6465\n",
      "Epoch 11/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6570 - accuracy: 0.6566\n",
      "Epoch 12/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6537 - accuracy: 0.6566\n",
      "Epoch 13/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6515 - accuracy: 0.6566\n",
      "Epoch 14/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6492 - accuracy: 0.6667\n",
      "Epoch 15/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.6667\n",
      "Epoch 16/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6429 - accuracy: 0.6667\n",
      "Epoch 17/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6433 - accuracy: 0.6869\n",
      "Epoch 18/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6422 - accuracy: 0.6768\n",
      "Epoch 19/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6424 - accuracy: 0.6667\n",
      "Epoch 20/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6413 - accuracy: 0.6667\n",
      "Epoch 21/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.6361 - accuracy: 0.6768\n",
      "Epoch 22/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6314 - accuracy: 0.6768\n",
      "Epoch 23/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6285 - accuracy: 0.6667\n",
      "Epoch 24/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.6667\n",
      "Epoch 25/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6350 - accuracy: 0.6667\n",
      "Epoch 26/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6268 - accuracy: 0.6465\n",
      "Epoch 27/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6224 - accuracy: 0.6667\n",
      "Epoch 28/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6241 - accuracy: 0.6869\n",
      "Epoch 29/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6328 - accuracy: 0.6667\n",
      "Epoch 30/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6347 - accuracy: 0.6566\n",
      "Epoch 31/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6343 - accuracy: 0.6667\n",
      "Epoch 32/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6322 - accuracy: 0.6667\n",
      "Epoch 33/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6290 - accuracy: 0.6667\n",
      "Epoch 34/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.6214 - accuracy: 0.6667\n",
      "Epoch 35/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6170 - accuracy: 0.6869\n",
      "Epoch 36/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6128 - accuracy: 0.6768\n",
      "Epoch 37/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6096 - accuracy: 0.6869\n",
      "Epoch 38/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6113 - accuracy: 0.6869\n",
      "Epoch 39/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6098 - accuracy: 0.6970\n",
      "Epoch 40/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6078 - accuracy: 0.7071\n",
      "Epoch 41/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6055 - accuracy: 0.6869\n",
      "Epoch 42/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6017 - accuracy: 0.6970\n",
      "Epoch 43/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6037 - accuracy: 0.7071\n",
      "Epoch 44/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6025 - accuracy: 0.6869\n",
      "Epoch 45/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.6064 - accuracy: 0.6667\n",
      "Epoch 46/200\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6019 - accuracy: 0.6768\n",
      "Epoch 47/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.6021 - accuracy: 0.6364\n",
      "Epoch 48/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5954 - accuracy: 0.6768\n",
      "Epoch 49/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5941 - accuracy: 0.6970\n",
      "Epoch 50/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5931 - accuracy: 0.7172\n",
      "Epoch 51/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5921 - accuracy: 0.7172\n",
      "Epoch 52/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5920 - accuracy: 0.7172\n",
      "Epoch 53/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5947 - accuracy: 0.7071\n",
      "Epoch 54/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5937 - accuracy: 0.7071\n",
      "Epoch 55/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5867 - accuracy: 0.7172\n",
      "Epoch 56/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5814 - accuracy: 0.7172\n",
      "Epoch 57/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5795 - accuracy: 0.6970\n",
      "Epoch 58/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.7172\n",
      "Epoch 59/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5766 - accuracy: 0.7172\n",
      "Epoch 60/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5756 - accuracy: 0.7172\n",
      "Epoch 61/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5792 - accuracy: 0.7071\n",
      "Epoch 62/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5736 - accuracy: 0.7273\n",
      "Epoch 63/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5692 - accuracy: 0.7071\n",
      "Epoch 64/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5836 - accuracy: 0.6970\n",
      "Epoch 65/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5765 - accuracy: 0.6869\n",
      "Epoch 66/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5647 - accuracy: 0.6970\n",
      "Epoch 67/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5611 - accuracy: 0.7071\n",
      "Epoch 68/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5607 - accuracy: 0.7172\n",
      "Epoch 69/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5571 - accuracy: 0.7273\n",
      "Epoch 70/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5592 - accuracy: 0.7071\n",
      "Epoch 71/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5845 - accuracy: 0.6667\n",
      "Epoch 72/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5598 - accuracy: 0.7071\n",
      "Epoch 73/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5639 - accuracy: 0.6869\n",
      "Epoch 74/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5687 - accuracy: 0.6970\n",
      "Epoch 75/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5536 - accuracy: 0.7172\n",
      "Epoch 76/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5420 - accuracy: 0.7071\n",
      "Epoch 77/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5396 - accuracy: 0.7172\n",
      "Epoch 78/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5271 - accuracy: 0.7374\n",
      "Epoch 79/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5463 - accuracy: 0.6970\n",
      "Epoch 80/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5537 - accuracy: 0.7071\n",
      "Epoch 81/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5294 - accuracy: 0.7677\n",
      "Epoch 82/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5644 - accuracy: 0.7475\n",
      "Epoch 83/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.7172\n",
      "Epoch 84/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5221 - accuracy: 0.7576\n",
      "Epoch 85/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5436 - accuracy: 0.7172\n",
      "Epoch 86/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5340 - accuracy: 0.7273\n",
      "Epoch 87/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5300 - accuracy: 0.7576\n",
      "Epoch 88/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5526 - accuracy: 0.7071\n",
      "Epoch 89/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5344 - accuracy: 0.7273\n",
      "Epoch 90/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5100 - accuracy: 0.7879\n",
      "Epoch 91/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5011 - accuracy: 0.7475\n",
      "Epoch 92/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5136 - accuracy: 0.7273\n",
      "Epoch 93/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5080 - accuracy: 0.7172\n",
      "Epoch 94/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5046 - accuracy: 0.7172\n",
      "Epoch 95/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5311 - accuracy: 0.7879\n",
      "Epoch 96/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.7879\n",
      "Epoch 97/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5258 - accuracy: 0.6970\n",
      "Epoch 98/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5199 - accuracy: 0.6970\n",
      "Epoch 99/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.7374\n",
      "Epoch 100/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4983 - accuracy: 0.7677\n",
      "Epoch 101/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5663 - accuracy: 0.6869\n",
      "Epoch 102/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4907 - accuracy: 0.7677\n",
      "Epoch 103/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5412 - accuracy: 0.7071\n",
      "Epoch 104/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5647 - accuracy: 0.6970\n",
      "Epoch 105/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4901 - accuracy: 0.7374\n",
      "Epoch 106/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5128 - accuracy: 0.7273\n",
      "Epoch 107/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5508 - accuracy: 0.7273\n",
      "Epoch 108/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4836 - accuracy: 0.7576\n",
      "Epoch 109/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5245 - accuracy: 0.6869\n",
      "Epoch 110/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.6970\n",
      "Epoch 111/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7677\n",
      "Epoch 112/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4641 - accuracy: 0.7980\n",
      "Epoch 113/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4629 - accuracy: 0.7475\n",
      "Epoch 114/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4748 - accuracy: 0.7273\n",
      "Epoch 115/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4614 - accuracy: 0.7475\n",
      "Epoch 116/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5060 - accuracy: 0.7677\n",
      "Epoch 117/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4589 - accuracy: 0.7879\n",
      "Epoch 118/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4802 - accuracy: 0.7475\n",
      "Epoch 119/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7879\n",
      "Epoch 120/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4842 - accuracy: 0.7677\n",
      "Epoch 121/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4779 - accuracy: 0.7778\n",
      "Epoch 122/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4528 - accuracy: 0.7677\n",
      "Epoch 123/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4474 - accuracy: 0.7980\n",
      "Epoch 124/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4450 - accuracy: 0.7980\n",
      "Epoch 125/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4505 - accuracy: 0.8081\n",
      "Epoch 126/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4513 - accuracy: 0.8081\n",
      "Epoch 127/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.7980\n",
      "Epoch 128/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4524 - accuracy: 0.7980\n",
      "Epoch 129/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4374 - accuracy: 0.8182\n",
      "Epoch 130/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.8283\n",
      "Epoch 131/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8182\n",
      "Epoch 132/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4366 - accuracy: 0.7980\n",
      "Epoch 133/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.8081\n",
      "Epoch 134/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.8182\n",
      "Epoch 135/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4311 - accuracy: 0.7879\n",
      "Epoch 136/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.7879\n",
      "Epoch 137/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4443 - accuracy: 0.7980\n",
      "Epoch 138/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4340 - accuracy: 0.7980\n",
      "Epoch 139/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.7879\n",
      "Epoch 140/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4573 - accuracy: 0.7677\n",
      "Epoch 141/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5043 - accuracy: 0.7172\n",
      "Epoch 142/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4372 - accuracy: 0.7980\n",
      "Epoch 143/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7475\n",
      "Epoch 144/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.5607 - accuracy: 0.7172\n",
      "Epoch 145/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4354 - accuracy: 0.7778\n",
      "Epoch 146/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4585 - accuracy: 0.7475\n",
      "Epoch 147/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4815 - accuracy: 0.7374\n",
      "Epoch 148/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4340 - accuracy: 0.7980\n",
      "Epoch 149/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4206 - accuracy: 0.7980\n",
      "Epoch 150/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.5285 - accuracy: 0.7172\n",
      "Epoch 151/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4422 - accuracy: 0.7980\n",
      "Epoch 152/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4186 - accuracy: 0.8283\n",
      "Epoch 153/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4092 - accuracy: 0.8081\n",
      "Epoch 154/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4210 - accuracy: 0.7879\n",
      "Epoch 155/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4054 - accuracy: 0.8384\n",
      "Epoch 156/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4020 - accuracy: 0.8283\n",
      "Epoch 157/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4006 - accuracy: 0.8283\n",
      "Epoch 158/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4263 - accuracy: 0.8081\n",
      "Epoch 159/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4148 - accuracy: 0.8283\n",
      "Epoch 160/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3988 - accuracy: 0.8081\n",
      "Epoch 161/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.3930 - accuracy: 0.8485\n",
      "Epoch 162/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4116 - accuracy: 0.8081\n",
      "Epoch 163/200\n",
      "4/4 [==============================] - 0s 5ms/step - loss: 0.4225 - accuracy: 0.8182\n",
      "Epoch 164/200\n",
      "4/4 [==============================] - 0s 6ms/step - loss: 0.4207 - accuracy: 0.7677\n",
      "Epoch 165/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.7172\n",
      "Epoch 166/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.7677\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3953 - accuracy: 0.8182\n",
      "Epoch 168/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3887 - accuracy: 0.8283\n",
      "Epoch 169/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4089 - accuracy: 0.8182\n",
      "Epoch 170/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4082 - accuracy: 0.8182\n",
      "Epoch 171/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4205 - accuracy: 0.7879\n",
      "Epoch 172/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4724 - accuracy: 0.7374\n",
      "Epoch 173/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4149 - accuracy: 0.7980\n",
      "Epoch 174/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3873 - accuracy: 0.8283\n",
      "Epoch 175/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8081\n",
      "Epoch 176/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.7778\n",
      "Epoch 177/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5140 - accuracy: 0.7172\n",
      "Epoch 178/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4455 - accuracy: 0.7576\n",
      "Epoch 179/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8283\n",
      "Epoch 180/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3773 - accuracy: 0.8384\n",
      "Epoch 181/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4613 - accuracy: 0.7576\n",
      "Epoch 182/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4306 - accuracy: 0.7576\n",
      "Epoch 183/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4620 - accuracy: 0.7677\n",
      "Epoch 184/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.5675 - accuracy: 0.6768\n",
      "Epoch 185/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4220 - accuracy: 0.8081\n",
      "Epoch 186/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4145 - accuracy: 0.7879\n",
      "Epoch 187/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8182\n",
      "Epoch 188/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3902 - accuracy: 0.8081\n",
      "Epoch 189/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.7879\n",
      "Epoch 190/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8182\n",
      "Epoch 191/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.4067 - accuracy: 0.8182\n",
      "Epoch 192/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4208 - accuracy: 0.7980\n",
      "Epoch 193/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3868 - accuracy: 0.8283\n",
      "Epoch 194/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8283\n",
      "Epoch 195/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8384\n",
      "Epoch 196/200\n",
      "4/4 [==============================] - 0s 4ms/step - loss: 0.3750 - accuracy: 0.8283\n",
      "Epoch 197/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8384\n",
      "Epoch 198/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.4018 - accuracy: 0.7980\n",
      "Epoch 199/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3882 - accuracy: 0.8182\n",
      "Epoch 200/200\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e5824bca0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This builds the model for the first time\n",
    "rede_neural.fit(X_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "455ecc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.76590896],\n",
       "       [0.95006025],\n",
       "       [0.9601191 ],\n",
       "       [0.9024396 ],\n",
       "       [0.64936495],\n",
       "       [0.8575261 ],\n",
       "       [0.8515617 ],\n",
       "       [0.78852785],\n",
       "       [0.8283708 ],\n",
       "       [0.62062466],\n",
       "       [0.84242886],\n",
       "       [0.1300022 ],\n",
       "       [0.5180168 ],\n",
       "       [0.8075011 ],\n",
       "       [0.7804109 ],\n",
       "       [0.3458643 ],\n",
       "       [0.64936495],\n",
       "       [0.9720639 ],\n",
       "       [0.01337599],\n",
       "       [0.76386917],\n",
       "       [0.84462214],\n",
       "       [0.8001571 ],\n",
       "       [0.41629407],\n",
       "       [0.95251864],\n",
       "       [0.9720639 ],\n",
       "       [0.7087071 ],\n",
       "       [0.64936495],\n",
       "       [0.02640076],\n",
       "       [0.89041626],\n",
       "       [0.14631069],\n",
       "       [0.52508634],\n",
       "       [0.8405549 ],\n",
       "       [0.01412575]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rede_neural.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f89a420e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = (y_pred >= 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0517fcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5,  5],\n",
       "       [ 2, 21]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "677c4c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(cm, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43d830ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.50      0.59        10\n",
      "           1       0.81      0.91      0.86        23\n",
      "\n",
      "    accuracy                           0.79        33\n",
      "   macro avg       0.76      0.71      0.72        33\n",
      "weighted avg       0.78      0.79      0.78        33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "# F1 - score estava 0,83 e foi para 0,9 quando corrigi o valor da taxa selic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bd7c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy .:  0.7878787878787878\n",
      "Precision :  0.8076923076923077\n",
      "Recall ...:  0.9130434782608695\n",
      "F1 Score .:  0.7226890756302521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "print(\"Accuracy .: \", accuracy_score(y_test, y_pred))  # Accuracy classification score.\n",
    "\n",
    "print(\"Precision : \", precision_score(y_test, y_pred))\n",
    "\n",
    "print(\"Recall ...: \", recall_score(y_test, y_pred))\n",
    "\n",
    "print(\"F1 Score .: \", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36985a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando o modelo\n",
    "# https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-predict-new-samples-with-your-keras-model.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fbfd6413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcd553af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "filepath = './saved_model'\n",
    "save_model(rede_neural, filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
